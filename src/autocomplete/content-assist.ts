import { type ILexingError, IToken, TokenType } from "chevrotain"
import { parser, parse as parseRaw } from "../parser/parser"
import { visitor } from "../parser/visitor"
import { QuestDBLexer, IdentifierKeyword } from "../parser/lexer"
import type { Statement } from "../parser/ast"
import { IDENTIFIER_KEYWORD_TOKENS } from "./token-classification"

// =============================================================================
// Constants
// =============================================================================

/**
 * When the token count exceeds this threshold, skip Chevrotain's
 * computeContentAssist (which is exponential on deeply nested CTEs)
 * and return IdentifierKeyword to trigger table/column suggestions.
 */
const CONTENT_ASSIST_TOKEN_LIMIT = 150

const WORD_BOUNDARY_CHARS = new Set([
  " ",
  "\n",
  "\t",
  "\r",
  "(",
  ")",
  ",",
  ";",
  ".",
  "=",
  "<",
  ">",
  "+",
  "-",
  "*",
  "/",
  "%",
  "'",
  '"',
  "|",
  "&",
  "^",
  "~",
  "!",
  "@",
  ":",
  "[",
  "]",
])

// =============================================================================
// Types
// =============================================================================

/**
 * A table reference found in the SQL query
 */
export interface TableRef {
  /** The table name */
  table: string
  /** Optional alias */
  alias?: string
}

/**
 * Result from content assist analysis
 */
export interface ContentAssistResult {
  /** Token types that are syntactically valid at the cursor position */
  nextTokenTypes: TokenType[]
  /** Tables/aliases found in the query (for column suggestions) */
  tablesInScope: TableRef[]
  /** The tokens before the cursor */
  tokensBefore: IToken[]
  /** Whether the cursor is in the middle of a word (partial token being typed) */
  isMidWord: boolean
  /** Any lexer errors */
  lexErrors: ILexingError[]
}

// =============================================================================
// Table Extraction from AST
// =============================================================================

/**
 * Extract table references from a parsed AST
 */
function normalizeTableName(value: unknown): string | undefined {
  if (!value) return undefined
  if (typeof value === "string") return value
  if (typeof value === "object" && value !== null) {
    const obj = value as Record<string, unknown>
    if (Array.isArray(obj.parts)) {
      // Use only the last part (table name) for schema lookup.
      // Schema-qualified names like "public.trades" should resolve to "trades"
      // since the schema columns map is keyed by bare table name.
      const parts = obj.parts as string[]
      return parts[parts.length - 1]
    }
    if (typeof obj.name === "string") return obj.name
  }
  return undefined
}

function extractTablesFromAst(ast: unknown): TableRef[] {
  const tables: TableRef[] = []
  const visited = new WeakSet()

  function visit(node: unknown) {
    if (!node || typeof node !== "object") return
    if (visited.has(node)) return
    visited.add(node)

    const n = node as Record<string, unknown>

    // Handle table references in FROM clause
    if (n.type === "tableRef") {
      const tableName = normalizeTableName(n.table ?? n.name)
      if (tableName) {
        tables.push({
          table: tableName,
          alias: n.alias as string | undefined,
        })
      }
    }

    // Handle FROM clause with table
    if (n.from) {
      const tableName = normalizeTableName(n.from)
      if (tableName) {
        const from = n.from as Record<string, unknown>
        tables.push({
          table: tableName,
          alias: from.alias as string | undefined,
        })
      } else {
        visit(n.from)
      }
    }

    // Handle JOIN clauses
    if (n.joins && Array.isArray(n.joins)) {
      for (const join of n.joins as Record<string, unknown>[]) {
        const joinTableName = normalizeTableName(join.table)
        if (joinTableName) {
          tables.push({
            table: joinTableName,
            alias: join.alias as string | undefined,
          })
        }
        visit(join)
      }
    }

    // Handle UPDATE table
    if (n.type === "update" && n.table) {
      const tableName = normalizeTableName(n.table)
      if (tableName) {
        tables.push({ table: tableName })
      }
    }

    // Handle INSERT INTO table
    if (n.type === "insert" && n.table) {
      const tableName = normalizeTableName(n.table)
      if (tableName) {
        tables.push({ table: tableName })
      }
    }

    // Recurse into child nodes
    for (const key of Object.keys(n)) {
      const child = n[key]
      if (Array.isArray(child)) {
        for (const item of child) {
          visit(item)
        }
      } else if (typeof child === "object" && child !== null) {
        visit(child)
      }
    }
  }

  if (Array.isArray(ast)) {
    for (const stmt of ast) {
      visit(stmt)
    }
  } else {
    visit(ast)
  }

  return tables
}

/**
 * Try to extract tables by parsing the query.
 * If parsing fails, try to extract from tokens.
 */
function extractTables(fullSql: string, tokens: IToken[]): TableRef[] {
  // First, try to parse and extract from AST
  try {
    const { cst } = parseRaw(fullSql)
    const ast = visitor.visit(cst) as Statement[]
    if (ast && ast.length > 0) {
      return extractTablesFromAst(ast)
    }
  } catch {
    // Parsing or visitor failed, fall through to token-based extraction
  }

  // Fallback: extract from tokens by looking for table name patterns
  // Handles malformed input where full AST extraction is not available.
  const tables: TableRef[] = []
  const TABLE_PREFIX_TOKENS = new Set(["From", "Join", "Update", "Into"])
  const isIdentifierLike = (token: IToken | undefined): token is IToken =>
    !!token &&
    (token.tokenType.name === "Identifier" ||
      token.tokenType.name === "QuotedIdentifier" ||
      IDENTIFIER_KEYWORD_TOKENS.has(token.tokenType.name))

  const tokenToNamePart = (token: IToken): string => {
    if (token.tokenType.name === "QuotedIdentifier") {
      return token.image.slice(1, -1)
    }
    return token.image
  }

  const readQualifiedName = (
    startIndex: number,
  ): { name: string; nextIndex: number } | null => {
    const first = tokens[startIndex]
    if (!isIdentifierLike(first)) return null

    const parts = [tokenToNamePart(first)]
    let i = startIndex + 1

    while (
      i + 1 < tokens.length &&
      tokens[i].tokenType.name === "Dot" &&
      isIdentifierLike(tokens[i + 1])
    ) {
      parts.push(tokenToNamePart(tokens[i + 1]))
      i += 2
    }

    // Use only the last part (table name) for schema lookup.
    // Schema-qualified names like "metrics.trades" resolve to "trades".
    return { name: parts[parts.length - 1], nextIndex: i }
  }

  for (let i = 0; i < tokens.length; i++) {
    if (!TABLE_PREFIX_TOKENS.has(tokens[i].tokenType.name)) continue

    const tableNameResult = readQualifiedName(i + 1)
    if (!tableNameResult) continue

    let alias: string | undefined
    let aliasStart = tableNameResult.nextIndex
    if (tokens[aliasStart]?.tokenType.name === "As") {
      aliasStart++
    }
    if (isIdentifierLike(tokens[aliasStart])) {
      alias = tokenToNamePart(tokens[aliasStart])
    }

    tables.push({
      table: tableNameResult.name,
      alias,
    })

    // Continue from where we consumed table/alias to avoid duplicate captures.
    i = alias ? aliasStart : tableNameResult.nextIndex - 1
  }

  return tables
}

// =============================================================================
// Main API
// =============================================================================

/**
 * Check if a ruleStack path represents the catch-all implicit SELECT or pivot
 * at the top-level statement dispatch. These are BACKTRACK alternatives that
 * computeContentAssist explores unconditionally, producing noise when a
 * specific statement rule already matched.
 *
 * Detects: statement → implicitSelectBody / implicitSelectStatement / pivotStatement (top-level catch-all)
 * Does NOT match nested uses (e.g., cteDefinition → implicitSelectBody).
 */
function isImplicitStatementPath(
  ruleStack: string[],
  implicitRules: Set<string>,
): boolean {
  for (let i = 0; i < ruleStack.length - 1; i++) {
    if (ruleStack[i] === "statement" && implicitRules.has(ruleStack[i + 1])) {
      return true
    }
  }
  return false
}

/**
 * Collapse a trailing qualified reference (ident.ident...ident) in the token
 * stream into a single identifier token. This allows re-computing suggestions
 * as if the user typed a simple column name, working around Chevrotain's
 * qualifiedStar path dominating the selectItem alternatives.
 *
 * Example: [SELECT, t, ., col] → [SELECT, col]
 */
/**
 * Check if a token type name represents an identifier-like token.
 * This includes plain Identifier, QuotedIdentifier, and any keyword
 * that has the IdentifierKeyword category (e.g., Timestamp, Index, Type).
 */
function isIdentifierLikeTokenName(name: string): boolean {
  return (
    name === "Identifier" ||
    name === "QuotedIdentifier" ||
    name === "IdentifierKeyword" ||
    IDENTIFIER_KEYWORD_TOKENS.has(name)
  )
}

function collapseTrailingQualifiedRef(tokens: IToken[]): IToken[] | null {
  if (tokens.length < 3) return null

  // Walk backwards from end: expect Identifier (. Identifier)+ pattern
  const i = tokens.length - 1
  const lastToken = tokens[i]
  if (!isIdentifierLikeTokenName(lastToken.tokenType.name)) {
    return null
  }

  // Walk backwards: Dot Identifier pairs
  let start = i
  while (start >= 2) {
    const maybeDot = tokens[start - 1]
    const maybeIdent = tokens[start - 2]
    if (maybeDot.tokenType.name !== "Dot") break
    if (!isIdentifierLikeTokenName(maybeIdent.tokenType.name)) break
    start -= 2
  }

  // Must have at least one Dot (i.e., start < i - 1) to be a qualified ref
  if (start >= i) return null

  // Replace tokens[start..i] with just the last identifier
  return [...tokens.slice(0, start), lastToken]
}

/**
 * Compute content assist suggestions, handling CTE context specially.
 *
 * Chevrotain's computeContentAssist at the "statement" level may only find
 * the insertStatement path when WITH is present, missing selectStatement and
 * updateStatement paths. This function detects that case and merges suggestions
 * from all WITH-capable statement types.
 */
function computeSuggestions(tokens: IToken[]): TokenType[] {
  // For large token sequences (deeply nested CTEs), Chevrotain's
  // computeContentAssist becomes exponentially slow. Fall back to a
  // generic set of suggestions to avoid freezing the editor.
  if (tokens.length > CONTENT_ASSIST_TOKEN_LIMIT) {
    return [IdentifierKeyword]
  }

  const ruleName = tokens.some((t) => t.tokenType.name === "Semicolon")
    ? "statements"
    : "statement"
  const suggestions = parser.computeContentAssist(ruleName, tokens)

  // Filter out noise from implicit SELECT / pivot catch-all paths.
  const IMPLICIT_RULES = new Set([
    "implicitSelectBody",
    "implicitSelectStatement",
    "pivotStatement",
  ])
  const specific = suggestions.filter(
    (s) => !isImplicitStatementPath(s.ruleStack, IMPLICIT_RULES),
  )
  const result = (specific.length > 0 ? specific : suggestions).map(
    (s) => s.nextTokenType,
  )

  // CTE fix: When tokens start with WITH, computeContentAssist at the
  // "statement" level only explores insertStatement (which comes first in
  // the OR). We also need suggestions from selectStatement and updateStatement.
  if (tokens.length > 0 && tokens[0].tokenType.name === "With") {
    const seen = new Set(result.map((t) => t.name))
    for (const rule of ["selectStatement", "updateStatement"]) {
      try {
        const extra = parser.computeContentAssist(rule, tokens)
        for (const s of extra) {
          if (!seen.has(s.nextTokenType.name)) {
            seen.add(s.nextTokenType.name)
            result.push(s.nextTokenType)
          }
        }
      } catch (e) {
        // Rule may not match — ignore
      }
    }
  }

  // qualifiedStar fix: When computeContentAssist finds the qualifiedStar
  // path in selectItem (suggesting just Dot), the expression path is missed.
  // Detect this by checking if the *specific* (non-catch-all) suggestions are
  // all from qualifiedStar, then re-compute with the qualified reference
  // collapsed to a single identifier to get expression-path suggestions.
  const effectiveSuggestions = specific.length > 0 ? specific : suggestions
  if (
    effectiveSuggestions.length > 0 &&
    effectiveSuggestions.every((s) => s.ruleStack.includes("qualifiedStar"))
  ) {
    // Find and collapse the trailing qualified reference (ident.ident...ident)
    // into a single identifier token, then re-compute to get expression-path suggestions.
    const collapsed = collapseTrailingQualifiedRef(tokens)
    if (collapsed) {
      try {
        const extra = parser.computeContentAssist(ruleName, collapsed)
        const filteredExtra = extra.filter(
          (s) => !isImplicitStatementPath(s.ruleStack, IMPLICIT_RULES),
        )
        const extraResult = (
          filteredExtra.length > 0 ? filteredExtra : extra
        ).map((s) => s.nextTokenType)
        const seen = new Set(result.map((t) => t.name))
        for (const t of extraResult) {
          if (!seen.has(t.name)) {
            seen.add(t.name)
            result.push(t)
          }
        }
      } catch (e) {
        // Ignore errors
      }
    }
  }

  return result
}

/**
 * When tablesInScope is empty (no FROM/JOIN yet), try to infer a table name
 * from a trailing qualified reference before the cursor.
 * e.g. "SELECT trades." → infer "trades" as table in scope.
 */
function inferTableFromQualifiedRef(
  tokensBefore: IToken[],
  isMidWord: boolean,
): TableRef | null {
  // If mid-word, the last token is a partial column name — look before it.
  const effective = isMidWord ? tokensBefore.slice(0, -1) : tokensBefore
  const lastIdx = effective.length - 1
  if (lastIdx < 1) return null
  if (effective[lastIdx].tokenType.name !== "Dot") return null

  const tableToken = effective[lastIdx - 1]
  if (!isIdentifierLikeTokenName(tableToken.tokenType.name)) return null

  const table =
    tableToken.tokenType.name === "QuotedIdentifier"
      ? tableToken.image.slice(1, -1)
      : tableToken.image

  return { table }
}

/**
 * Get content assist suggestions for a SQL string at a given cursor position
 *
 * @param fullSql - The complete SQL string
 * @param cursorOffset - The cursor position (0-indexed character offset)
 * @returns Content assist result with next valid tokens and tables in scope
 */
export function getContentAssist(
  fullSql: string,
  cursorOffset: number,
): ContentAssistResult {
  // Tokenize the full SQL to check if the cursor is inside a string literal.
  // If so, suppress suggestions — the user is editing a value, not SQL syntax.
  // This also covers single-quoted identifiers (FROM 'table'), which is an
  // acceptable trade-off to avoid fragile context-detection heuristics.
  const fullTokens = QuestDBLexer.tokenize(fullSql).tokens
  for (const token of fullTokens) {
    if (token.tokenType.name !== "StringLiteral") continue
    const start = token.startOffset
    const end = token.startOffset + token.image.length
    if (cursorOffset > start && cursorOffset < end) {
      return {
        nextTokenTypes: [],
        tablesInScope: [],
        tokensBefore: [],
        isMidWord: true,
        lexErrors: [],
      }
    }
  }

  // Split the text at cursor position
  const beforeCursor = fullSql.substring(0, cursorOffset)

  // Tokenize text before cursor
  const lexResult = QuestDBLexer.tokenize(beforeCursor)
  const tokens = lexResult.tokens

  // Check if cursor is mid-word (last char before cursor is not whitespace/punctuation).
  // If so, the last token is a partial word the user is still typing.
  // Drop it before calling computeContentAssist so the parser tells us what's
  // valid at the position BEFORE that word. The caller filters by the prefix.
  // Structural characters like ( ) , ; are NOT mid-word — they are complete tokens.
  const lastChar =
    cursorOffset > 0 && cursorOffset <= fullSql.length
      ? fullSql[cursorOffset - 1]
      : " "
  const isMidWord = !WORD_BOUNDARY_CHARS.has(lastChar)
  const tokensForAssist =
    isMidWord && tokens.length > 0 ? tokens.slice(0, -1) : tokens

  // Get syntactically valid next tokens using Chevrotain's content assist
  let nextTokenTypes: TokenType[] = []
  try {
    nextTokenTypes = computeSuggestions(tokensForAssist)
  } catch (e) {
    // If content assist fails, return empty suggestions
    // This can happen with malformed input
  }

  // Extract tables from the full query (reuses fullTokens from above)
  const tablesInScope = extractTables(fullSql, fullTokens)

  if (tablesInScope.length === 0) {
    const inferred = inferTableFromQualifiedRef(tokens, isMidWord)
    if (inferred) tablesInScope.push(inferred)
  }

  return {
    nextTokenTypes,
    tablesInScope,
    tokensBefore: tokens,
    isMidWord,
    lexErrors: lexResult.errors,
  }
}

/**
 * Simplified version that just returns next valid token names
 */
export function getNextValidTokens(sql: string): string[] {
  const lexResult = QuestDBLexer.tokenize(sql)
  try {
    return computeSuggestions(lexResult.tokens).map((t) => t.name)
  } catch (e) {
    return []
  }
}

/**
 * Check if a token type is expected at the current position
 */
export function isTokenExpected(sql: string, tokenName: string): boolean {
  const validTokens = getNextValidTokens(sql)
  return validTokens.includes(tokenName)
}
